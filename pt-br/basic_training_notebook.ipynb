{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r11cNiLqvWC6"
   },
   "source": [
    "# Training a microWakeWord Model\n",
    "\n",
    "This notebook steps you through training a basic microWakeWord model. It is intended as a **starting point** for advanced users. You should use Python 3.10.\n",
    "\n",
    "**The model generated will most likely not be usable for everyday use; it may be difficult to trigger or falsely activates too frequently. You will most likely have to experiment with many different settings to obtain a decent model!**\n",
    "\n",
    "In the comment at the start of certain blocks, I note some specific settings to consider modifying.\n",
    "\n",
    "This runs on Google Colab, but is extremely slow compared to training on a local GPU. If you must use Colab, be sure to Change the runtime type to a GPU. Even then, it still slow!\n",
    "\n",
    "At the end of this notebook, you will be able to download a tflite file. To use this in ESPHome, you need to write a model manifest JSON file. See the [ESPHome documentation](https://esphome.io/components/micro_wake_word) for the details and the [model repo](https://github.com/esphome/micro-wake-word-models/tree/main/models/v2) for examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFf6511E65ff"
   },
   "outputs": [],
   "source": [
    "# Installs microWakeWord. Be sure to restart the session after this is finished.\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    # `pymicro-features` is installed from a fork to support building on macOS\n",
    "    !pip install 'git+https://github.com/puddly/pymicro-features@puddly/minimum-cpp-version'\n",
    "\n",
    "# `audio-metadata` is installed from a fork to unpin `attrs` from a version that breaks Jupyter\n",
    "!pip install 'git+https://github.com/whatsnowplaying/audio-metadata@d4ebb238e6a401bb1a5aaaac60c9e2b3cb30929f'\n",
    "\n",
    "!git clone -b november-update https://github.com/kahrendt/microWakeWord\n",
    "!pip install -e ./microWakeWord\n",
    "\n",
    "# --- Patch microWakeWord for TF 2.20+ compatibility ---\n",
    "# model.evaluate() now returns plain numpy values instead of tf.Tensor,\n",
    "# so .numpy() calls on metric results fail. Add a safe conversion helper.\n",
    "import pathlib\n",
    "_train_py = pathlib.Path('microWakeWord/microwakeword/train.py')\n",
    "_src = _train_py.read_text()\n",
    "if \"_to_numpy\" not in _src:\n",
    "    _src = _src.replace(\n",
    "        'test_set_fp = result[\"fp\"].numpy()',\n",
    "        'test_set_fp = result[\"fp\"].numpy() if hasattr(result[\"fp\"], \"numpy\") else np.asarray(result[\"fp\"])'\n",
    "    ).replace(\n",
    "        'all_true_positives = ambient_predictions[\"tp\"].numpy()',\n",
    "        'all_true_positives = ambient_predictions[\"tp\"].numpy() if hasattr(ambient_predictions[\"tp\"], \"numpy\") else np.asarray(ambient_predictions[\"tp\"])'\n",
    "    ).replace(\n",
    "        'ambient_false_positives = ambient_predictions[\"fp\"].numpy() - test_set_fp',\n",
    "        'ambient_false_positives = (ambient_predictions[\"fp\"].numpy() if hasattr(ambient_predictions[\"fp\"], \"numpy\") else np.asarray(ambient_predictions[\"fp\"])) - test_set_fp'\n",
    "    ).replace(\n",
    "        'all_false_negatives = ambient_predictions[\"fn\"].numpy()',\n",
    "        'all_false_negatives = ambient_predictions[\"fn\"].numpy() if hasattr(ambient_predictions[\"fn\"], \"numpy\") else np.asarray(ambient_predictions[\"fn\"])'\n",
    "    )\n",
    "    # Mark as patched\n",
    "    _src = '# _to_numpy patch applied\\n' + _src\n",
    "    _train_py.write_text(_src)\n",
    "    print('Patched train.py for TF 2.20+ .numpy() compat')\n",
    "else:\n",
    "    print('train.py already patched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEluu7nL7ywd"
   },
   "outputs": [],
   "source": [
    "# Generates 1 sample per voice/spelling combo for manual verification.\n",
    "# For pt-BR, we download Piper checkpoints and export the generator models.\n",
    "# The default .pt release only supports English, so we convert pt_BR checkpoints.\n",
    "\n",
    "# === CONFIGURE YOUR WAKE WORD SPELLINGS AND VOICES HERE ===\n",
    "# Add/remove spellings to cover pronunciation variations.\n",
    "# Add/remove voices to get different speaker characteristics.\n",
    "# Each spelling produces a different phoneme sequence via espeak-ng.\n",
    "target_spellings = [\n",
    "    'Ei Sexta!',     # canonical pronunciation\n",
    "    'Oi Sexta',      # \"oi\" instead of \"ei\" (common casual variant)\n",
    "    'Ei sêxtá',      # stress on both syllables of \"sexta\"\n",
    "]\n",
    "\n",
    "voices = {\n",
    "    'faber': {\n",
    "        'ckpt_url': 'https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/pt/pt_BR/faber/medium/epoch%3D6159-step%3D1230728.ckpt',\n",
    "        'config_url': 'https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/pt/pt_BR/faber/medium/config.json',\n",
    "    },\n",
    "    'cadu': {\n",
    "        'ckpt_url': 'https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/pt/pt_BR/cadu/medium/epoch%3D5195-step%3D109116.ckpt',\n",
    "        'config_url': 'https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/pt/pt_BR/cadu/medium/config.json',\n",
    "    },\n",
    "    'jeff': {\n",
    "        'ckpt_url': 'https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/pt/pt_BR/jeff/medium/epoch%3D5462-step%3D118728.ckpt',\n",
    "        'config_url': 'https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/pt/pt_BR/jeff/medium/config.json',\n",
    "    },\n",
    "}\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import shutil\n",
    "\n",
    "from IPython.display import Audio, display, HTML\n",
    "\n",
    "if not os.path.exists('./piper-sample-generator'):\n",
    "    if platform.system() == 'Darwin':\n",
    "        !git clone -b mps-support https://github.com/kahrendt/piper-sample-generator\n",
    "    else:\n",
    "        !git clone https://github.com/rhasspy/piper-sample-generator\n",
    "\n",
    "    # Install system dependencies\n",
    "    !pip install -q torch torchaudio piper-phonemize-cross==1.2.1 pytorch-lightning\n",
    "\n",
    "    # Patch generate_samples.py for PyTorch 2.6+ (weights_only default changed to True)\n",
    "    # and for single-speaker models (no emb_g attribute)\n",
    "    gen_script = 'piper-sample-generator/generate_samples.py'\n",
    "    with open(gen_script, 'r') as f:\n",
    "        content = f.read()\n",
    "    content = content.replace('torch.load(model_path)', 'torch.load(model_path, weights_only=False)')\n",
    "    content = content.replace(\n",
    "        'emb0 = model.emb_g(speaker_1)\\n    emb1 = model.emb_g(speaker_2)\\n    g = slerp(emb0, emb1, slerp_weight).unsqueeze(-1)  # [b, h, 1]',\n",
    "        'if hasattr(model, \\'emb_g\\'):\\n        emb0 = model.emb_g(speaker_1)\\n        emb1 = model.emb_g(speaker_2)\\n        g = slerp(emb0, emb1, slerp_weight).unsqueeze(-1)  # [b, h, 1]\\n    else:\\n        g = None'\n",
    "    )\n",
    "    with open(gen_script, 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "if 'piper-sample-generator/' not in sys.path:\n",
    "    sys.path.append('piper-sample-generator/')\n",
    "\n",
    "# Download and export each voice model (idempotent — skips if .pt already exists)\n",
    "import torch\n",
    "import pathlib\n",
    "\n",
    "models_dir = 'piper-sample-generator/models'\n",
    "model_paths = {}  # voice_name -> .pt path\n",
    "\n",
    "for voice_name, voice_info in voices.items():\n",
    "    pt_path = os.path.join(models_dir, f'pt_BR-{voice_name}-medium.pt')\n",
    "    json_path = pt_path + '.json'\n",
    "    model_paths[voice_name] = pt_path\n",
    "\n",
    "    if os.path.exists(pt_path):\n",
    "        print(f'{voice_name}: model already exported at {pt_path}')\n",
    "        continue\n",
    "\n",
    "    ckpt_path = os.path.join(models_dir, f'pt_BR-{voice_name}-medium.ckpt')\n",
    "    print(f'{voice_name}: downloading checkpoint...')\n",
    "    !curl -L -o {ckpt_path} '{voice_info[\"ckpt_url\"]}'\n",
    "    !curl -L -o {json_path} '{voice_info[\"config_url\"]}'\n",
    "\n",
    "    torch.serialization.add_safe_globals([pathlib.PosixPath, pathlib.WindowsPath])\n",
    "\n",
    "    # Manually reconstruct the generator from the checkpoint to avoid\n",
    "    # Lightning version incompatibilities with load_from_checkpoint.\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "    hparams = ckpt['hyper_parameters']\n",
    "\n",
    "    from piper_train.vits.models import SynthesizerTrn\n",
    "    model_g = SynthesizerTrn(\n",
    "        n_vocab=hparams['num_symbols'],\n",
    "        spec_channels=hparams.get('filter_length', 1024) // 2 + 1,\n",
    "        segment_size=hparams.get('segment_size', 8192) // hparams.get('hop_length', 256),\n",
    "        inter_channels=hparams.get('inter_channels', 192),\n",
    "        hidden_channels=hparams.get('hidden_channels', 192),\n",
    "        filter_channels=hparams.get('filter_channels', 768),\n",
    "        n_heads=hparams.get('n_heads', 2),\n",
    "        n_layers=hparams.get('n_layers', 6),\n",
    "        kernel_size=hparams.get('kernel_size', 3),\n",
    "        p_dropout=hparams.get('p_dropout', 0.1),\n",
    "        resblock=hparams.get('resblock', '2'),\n",
    "        resblock_kernel_sizes=hparams.get('resblock_kernel_sizes', (3, 5, 7)),\n",
    "        resblock_dilation_sizes=hparams.get('resblock_dilation_sizes', ((1,2),(2,6),(3,12))),\n",
    "        upsample_rates=hparams.get('upsample_rates', (8, 8, 4)),\n",
    "        upsample_initial_channel=hparams.get('upsample_initial_channel', 256),\n",
    "        upsample_kernel_sizes=hparams.get('upsample_kernel_sizes', (16, 16, 8)),\n",
    "        n_speakers=hparams.get('num_speakers', 1),\n",
    "        gin_channels=hparams.get('gin_channels', 0),\n",
    "        use_sdp=hparams.get('use_sdp', True),\n",
    "    )\n",
    "\n",
    "    # Load only the generator weights from the checkpoint state_dict\n",
    "    g_state = {k.replace('model_g.', ''): v for k, v in ckpt['state_dict'].items() if k.startswith('model_g.')}\n",
    "    model_g.load_state_dict(g_state)\n",
    "    model_g.eval()\n",
    "    torch.save(model_g, pt_path)\n",
    "    del ckpt, g_state, model_g  # free memory\n",
    "    os.remove(ckpt_path)\n",
    "    print(f'{voice_name}: exported successfully')\n",
    "\n",
    "# Generate 1 sample per voice × spelling combo and play them all\n",
    "preview_dir = 'generated_preview'\n",
    "if os.path.exists(preview_dir):\n",
    "    shutil.rmtree(preview_dir)\n",
    "os.makedirs(preview_dir, exist_ok=True)\n",
    "\n",
    "sample_idx = 0\n",
    "for voice_name, pt_path in model_paths.items():\n",
    "    for spelling in target_spellings:\n",
    "        out_dir = os.path.join(preview_dir, f'{voice_name}_{sample_idx}')\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        !python3 piper-sample-generator/generate_samples.py \"{spelling}\" \\\n",
    "            --model {pt_path} \\\n",
    "            --max-samples 1 \\\n",
    "            --batch-size 1 \\\n",
    "            --output-dir {out_dir}\n",
    "        wav_path = os.path.join(out_dir, '0.wav')\n",
    "        display(HTML(f'<b>{voice_name}</b> — \"{spelling}\"'))\n",
    "        display(Audio(wav_path))\n",
    "        sample_idx += 1\n",
    "\n",
    "print(f'\\nGenerated {sample_idx} preview samples across {len(model_paths)} voices and {len(target_spellings)} spellings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SvGtCCM9akR"
   },
   "outputs": [],
   "source": [
    "# Generates a larger amount of wake word samples from ALL voices and spellings.\n",
    "# Uses the voices/spellings defined in cell 2 — run that cell first.\n",
    "#\n",
    "# Key parameters:\n",
    "#   --length-scales: speaking speed (lower=faster, higher=slower).\n",
    "#   --noise-scales: overall variability/expressiveness.\n",
    "#   --noise-scale-ws: stochastic duration variation of individual phonemes.\n",
    "#   --max-samples: more samples = better model. ~1000+ per combo recommended.\n",
    "\n",
    "import os, shutil, math, glob, subprocess\n",
    "\n",
    "output_dir = 'generated_samples'\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "total_samples = 10000\n",
    "num_combos = len(model_paths) * len(target_spellings)\n",
    "samples_per_combo = math.ceil(total_samples / num_combos)\n",
    "\n",
    "print(f'Generating ~{total_samples} total samples ({samples_per_combo} per combo, {num_combos} combos)')\n",
    "\n",
    "file_idx = 0\n",
    "for voice_name, pt_path in model_paths.items():\n",
    "    for spelling in target_spellings:\n",
    "        # Generate into a temp dir to avoid filename collisions\n",
    "        tmp_dir = os.path.join(output_dir, f'_tmp_{voice_name}_{id(spelling)}')\n",
    "        os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "        print(f'\\n>>> {voice_name} — \"{spelling}\" ({samples_per_combo} samples)')\n",
    "        subprocess.run([\n",
    "            'python3', 'piper-sample-generator/generate_samples.py', spelling,\n",
    "            '--model', pt_path,\n",
    "            '--max-samples', str(samples_per_combo),\n",
    "            '--batch-size', '100',\n",
    "            '--length-scales', '0.6', '0.75', '0.85', '1.0', '1.15', '1.3', '1.5',\n",
    "            '--noise-scales', '0.5', '0.667', '0.75', '0.85', '1.0', '1.2',\n",
    "            '--noise-scale-ws', '0.6', '0.8', '1.0', '1.2',\n",
    "            '--output-dir', tmp_dir,\n",
    "        ], check=True)\n",
    "\n",
    "        # Move files into the main output dir with unique sequential names\n",
    "        for wav in sorted(glob.glob(os.path.join(tmp_dir, '*.wav'))):\n",
    "            os.rename(wav, os.path.join(output_dir, f'{file_idx}.wav'))\n",
    "            file_idx += 1\n",
    "        shutil.rmtree(tmp_dir)\n",
    "\n",
    "print(f'\\nDone! {file_idx} samples in {output_dir}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJRG4Qvo9nXG"
   },
   "outputs": [],
   "source": [
    "# Downloads audio data for augmentation. This can be slow!\n",
    "# Borrowed from openWakeWord's automatic_model_training.ipynb, accessed March 4, 2024\n",
    "#\n",
    "# datasets 4.0+ replaced soundfile with torchcodec and changed the audio API.\n",
    "# We need <4.0.0 so the dict-based audio decoding (row['audio']['array']) works.\n",
    "import datasets as _ds_check\n",
    "if _ds_check.__version__ >= '4.0.0':\n",
    "    import subprocess, sys, IPython\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'datasets<4.0.0'])\n",
    "    print('\\n⚠️  datasets was downgraded. Restarting kernel — just re-run this cell.')\n",
    "    IPython.get_ipython().kernel.do_shutdown(restart=True)\n",
    "    raise SystemExit  # stop execution until kernel restarts\n",
    "del _ds_check\n",
    "\n",
    "#\n",
    "# **Important note!** The data downloaded here has a mixture of difference\n",
    "# licenses and usage restrictions. As such, any custom models trained with this\n",
    "# data should be considered as appropriate for **non-commercial** personal use only.\n",
    "\n",
    "\n",
    "import datasets\n",
    "import scipy\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Download MIT RIR data\n",
    "\n",
    "output_dir = \"./mit_rirs\"\n",
    "if not glob.glob(os.path.join(output_dir, '*.wav')):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    rir_dataset = datasets.load_dataset(\"davidscripka/MIT_environmental_impulse_responses\", split=\"train\", streaming=True)\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    for row in tqdm(rir_dataset):\n",
    "        name = row['audio']['path'].split('/')[-1]\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "else:\n",
    "    print(f'MIT RIRs already downloaded ({len(glob.glob(os.path.join(output_dir, \"*.wav\")))} files)')\n",
    "\n",
    "## Download noise and background audio\n",
    "\n",
    "# Audioset Dataset (https://research.google.com/audioset/dataset/index.html)\n",
    "# Download one part of the audioset .tar files, extract, and convert to 16khz\n",
    "# For full-scale training, it's recommended to download the entire dataset from\n",
    "# https://huggingface.co/datasets/agkphysics/AudioSet, and\n",
    "# even potentially combine it with other background noise datasets (e.g., FSD50k, Freesound, etc.)\n",
    "\n",
    "audioset_16k_dir = \"./audioset_16k\"\n",
    "if not glob.glob(os.path.join(audioset_16k_dir, '*.wav')):\n",
    "    os.makedirs(\"audioset\", exist_ok=True)\n",
    "\n",
    "    fname = \"bal_train09.tar\"\n",
    "    out_dir = f\"audioset/{fname}\"\n",
    "    link = \"https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/\" + fname\n",
    "    !curl -L -o {out_dir} {link}\n",
    "    !tar -xf audioset/bal_train09.tar -C audioset\n",
    "\n",
    "    os.makedirs(audioset_16k_dir, exist_ok=True)\n",
    "\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"audioset/audio\").glob(\"**/*.flac\")]})\n",
    "    audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "    for row in tqdm(audioset_dataset):\n",
    "        name = row['audio']['path'].split('/')[-1].replace(\".flac\", \".wav\")\n",
    "        scipy.io.wavfile.write(os.path.join(audioset_16k_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "else:\n",
    "    print(f'Audioset already converted ({len(glob.glob(os.path.join(audioset_16k_dir, \"*.wav\")))} files)')\n",
    "\n",
    "# Free Music Archive dataset\n",
    "# https://github.com/mdeff/fma\n",
    "# (Third-party mchl914 extra small set)\n",
    "\n",
    "fma_16k_dir = \"./fma_16k\"\n",
    "if not glob.glob(os.path.join(fma_16k_dir, '*.wav')):\n",
    "    os.makedirs(\"fma\", exist_ok=True)\n",
    "    fname = \"fma_xs.zip\"\n",
    "    link = \"https://huggingface.co/datasets/mchl914/fma_xsmall/resolve/main/\" + fname\n",
    "    out_dir = f\"fma/{fname}\"\n",
    "    !curl -L -o {out_dir} {link}\n",
    "    !unzip -q fma/{fname} -d fma\n",
    "\n",
    "    os.makedirs(fma_16k_dir, exist_ok=True)\n",
    "\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    fma_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"fma/fma_small\").glob(\"**/*.mp3\")]})\n",
    "    fma_dataset = fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "    for row in tqdm(fma_dataset):\n",
    "        name = row['audio']['path'].split('/')[-1].replace(\".mp3\", \".wav\")\n",
    "        scipy.io.wavfile.write(os.path.join(fma_16k_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "else:\n",
    "    print(f'FMA already converted ({len(glob.glob(os.path.join(fma_16k_dir, \"*.wav\")))} files)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XW3bmbI5-JAz"
   },
   "outputs": [],
   "source": [
    "# Sets up the augmentations.\n",
    "# To improve your model, experiment with these settings and use more sources of\n",
    "# background clips.\n",
    "\n",
    "from microwakeword.audio.augmentation import Augmentation\n",
    "from microwakeword.audio.clips import Clips\n",
    "from microwakeword.audio.spectrograms import SpectrogramGeneration\n",
    "\n",
    "clips = Clips(input_directory='generated_samples',\n",
    "              file_pattern='*.wav',\n",
    "              max_clip_duration_s=None,\n",
    "              remove_silence=False,\n",
    "              random_split_seed=10,\n",
    "              split_count=0.1,\n",
    "              )\n",
    "augmenter = Augmentation(augmentation_duration_s=3.2,\n",
    "                         augmentation_probabilities = {\n",
    "                                \"SevenBandParametricEQ\": 0.1,\n",
    "                                \"TanhDistortion\": 0.1,\n",
    "                                \"PitchShift\": 0.1,\n",
    "                                \"BandStopFilter\": 0.1,\n",
    "                                \"AddColorNoise\": 0.1,\n",
    "                                \"AddBackgroundNoise\": 0.75,\n",
    "                                \"Gain\": 1.0,\n",
    "                                \"RIR\": 0.5,\n",
    "                            },\n",
    "                         impulse_paths = ['mit_rirs'],\n",
    "                         background_paths = ['fma_16k', 'audioset_16k'],\n",
    "                         background_min_snr_db = -5,\n",
    "                         background_max_snr_db = 10,\n",
    "                         min_jitter_s = 0.195,\n",
    "                         max_jitter_s = 0.205,\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5UsJfKKD1k9"
   },
   "outputs": [],
   "source": [
    "# Augment a random clip and play it back to verify it works well\n",
    "\n",
    "from IPython.display import Audio\n",
    "from microwakeword.audio.audio_utils import save_clip\n",
    "\n",
    "random_clip = clips.get_random_clip()\n",
    "augmented_clip = augmenter.augment_clip(random_clip)\n",
    "save_clip(augmented_clip, 'augmented_clip.wav')\n",
    "\n",
    "Audio(\"augmented_clip.wav\", autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7BHcY1mEGbK"
   },
   "outputs": [],
   "source": [
    "# Augment samples and save the training, validation, and testing sets.\n",
    "# Validating and testing samples generated the same way can make the model\n",
    "# benchmark better than it performs in real-word use. Use real samples or TTS\n",
    "# samples generated with a different TTS engine to potentially get more accurate\n",
    "# benchmarks.\n",
    "\n",
    "import os\n",
    "from mmap_ninja.ragged import RaggedMmap\n",
    "\n",
    "output_dir = 'generated_augmented_features'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "splits = [\"training\", \"validation\", \"testing\"]\n",
    "for split in splits:\n",
    "  out_dir = os.path.join(output_dir, split)\n",
    "  if not os.path.exists(out_dir):\n",
    "      os.mkdir(out_dir)\n",
    "\n",
    "\n",
    "  split_name = \"train\"\n",
    "  repetition = 2\n",
    "\n",
    "  spectrograms = SpectrogramGeneration(clips=clips,\n",
    "                                     augmenter=augmenter,\n",
    "                                     slide_frames=10,    # Uses the same spectrogram repeatedly, just shifted over by one frame. This simulates the streaming inferences while training/validating in nonstreaming mode.\n",
    "                                     step_ms=10,\n",
    "                                     )\n",
    "  if split == \"validation\":\n",
    "    split_name = \"validation\"\n",
    "    repetition = 1\n",
    "  elif split == \"testing\":\n",
    "    split_name = \"test\"\n",
    "    repetition = 1\n",
    "    spectrograms = SpectrogramGeneration(clips=clips,\n",
    "                                     augmenter=augmenter,\n",
    "                                     slide_frames=1,    # The testing set uses the streaming version of the model, so no artificial repetition is necessary\n",
    "                                     step_ms=10,\n",
    "                                     )\n",
    "\n",
    "  RaggedMmap.from_generator(\n",
    "      out_dir=os.path.join(out_dir, 'wakeword_mmap'),\n",
    "      sample_generator=spectrograms.spectrogram_generator(split=split_name, repeat=repetition),\n",
    "      batch_size=100,\n",
    "      verbose=True,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pGuJDPyp3ax"
   },
   "outputs": [],
   "source": [
    "# Downloads pre-generated spectrogram features (made for microWakeWord in\n",
    "# particular) for various negative datasets. This can be slow!\n",
    "\n",
    "output_dir = './negative_datasets'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    link_root = \"https://huggingface.co/datasets/kahrendt/microwakeword/resolve/main/\"\n",
    "    filenames = ['dinner_party.zip', 'dinner_party_eval.zip', 'no_speech.zip', 'speech.zip']\n",
    "    for fname in filenames:\n",
    "        link = link_root + fname\n",
    "\n",
    "        zip_path = f\"negative_datasets/{fname}\"\n",
    "        !curl -L -o {zip_path} {link}\n",
    "        !unzip -q {zip_path} -d {output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ii1A14GsGVQT"
   },
   "outputs": [],
   "source": [
    "# Save a yaml config that controls the training process\n",
    "# These hyperparamters can make a huge different in model quality.\n",
    "# Experiment with sampling and penalty weights and increasing the number of\n",
    "# training steps.\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "config = {}\n",
    "\n",
    "config[\"window_step_ms\"] = 10\n",
    "\n",
    "config[\"train_dir\"] = (\n",
    "    \"trained_models/wakeword\"\n",
    ")\n",
    "\n",
    "\n",
    "# Each feature_dir should have at least one of the following folders with this structure:\n",
    "#  training/\n",
    "#    ragged_mmap_folders_ending_in_mmap\n",
    "#  testing/\n",
    "#    ragged_mmap_folders_ending_in_mmap\n",
    "#  testing_ambient/\n",
    "#    ragged_mmap_folders_ending_in_mmap\n",
    "#  validation/\n",
    "#    ragged_mmap_folders_ending_in_mmap\n",
    "#  validation_ambient/\n",
    "#    ragged_mmap_folders_ending_in_mmap\n",
    "#\n",
    "#  sampling_weight: Weight for choosing a spectrogram from this set in the batch\n",
    "#  penalty_weight: Penalizing weight for incorrect predictions from this set\n",
    "#  truth: Boolean whether this set has positive samples or negative samples\n",
    "#  truncation_strategy = If spectrograms in the set are longer than necessary for training, how are they truncated\n",
    "#       - random: choose a random portion of the entire spectrogram - useful for long negative samples\n",
    "#       - truncate_start: remove the start of the spectrogram\n",
    "#       - truncate_end: remove the end of the spectrogram\n",
    "#       - split: Split the longer spectrogram into separate spectrograms offset by 100 ms. Only for ambient sets\n",
    "\n",
    "config[\"features\"] = [\n",
    "    {\n",
    "        \"features_dir\": \"generated_augmented_features\",\n",
    "        \"sampling_weight\": 2.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": True,\n",
    "        \"truncation_strategy\": \"truncate_start\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        \"features_dir\": \"negative_datasets/speech\",\n",
    "        \"sampling_weight\": 10.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"random\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        \"features_dir\": \"negative_datasets/dinner_party\",\n",
    "        \"sampling_weight\": 10.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"random\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        \"features_dir\": \"negative_datasets/no_speech\",\n",
    "        \"sampling_weight\": 5.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"random\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    { # Only used for validation and testing\n",
    "        \"features_dir\": \"negative_datasets/dinner_party_eval\",\n",
    "        \"sampling_weight\": 0.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"split\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Number of training steps in each iteration - various other settings are configured as lists that corresponds to different steps\n",
    "config[\"training_steps\"] = [10000]\n",
    "\n",
    "# Penalizing weight for incorrect class predictions - lists that correspond to training steps\n",
    "config[\"positive_class_weight\"] = [1]\n",
    "config[\"negative_class_weight\"] = [20]\n",
    "\n",
    "config[\"learning_rates\"] = [\n",
    "    0.001,\n",
    "]  # Learning rates for Adam optimizer - list that corresponds to training steps\n",
    "config[\"batch_size\"] = 128\n",
    "\n",
    "config[\"time_mask_max_size\"] = [\n",
    "    0\n",
    "]  # SpecAugment - list that corresponds to training steps\n",
    "config[\"time_mask_count\"] = [0]  # SpecAugment - list that corresponds to training steps\n",
    "config[\"freq_mask_max_size\"] = [\n",
    "    0\n",
    "]  # SpecAugment - list that corresponds to training steps\n",
    "config[\"freq_mask_count\"] = [0]  # SpecAugment - list that corresponds to training steps\n",
    "\n",
    "config[\"eval_step_interval\"] = (\n",
    "    500  # Test the validation sets after every this many steps\n",
    ")\n",
    "config[\"clip_duration_ms\"] = (\n",
    "    1500  # Maximum length of wake word that the streaming model will accept\n",
    ")\n",
    "\n",
    "# The best model weights are chosen first by minimizing the specified minimization metric below the specified target_minimization\n",
    "# Once the target has been met, it chooses the maximum of the maximization metric. Set 'minimization_metric' to None to only maximize\n",
    "# Available metrics:\n",
    "#   - \"loss\" - cross entropy error on validation set\n",
    "#   - \"accuracy\" - accuracy of validation set\n",
    "#   - \"recall\" - recall of validation set\n",
    "#   - \"precision\" - precision of validation set\n",
    "#   - \"false_positive_rate\" - false positive rate of validation set\n",
    "#   - \"false_negative_rate\" - false negative rate of validation set\n",
    "#   - \"ambient_false_positives\" - count of false positives from the split validation_ambient set\n",
    "#   - \"ambient_false_positives_per_hour\" - estimated number of false positives per hour on the split validation_ambient set\n",
    "config[\"target_minimization\"] = 0.9\n",
    "config[\"minimization_metric\"] = None  # Set to None to disable\n",
    "\n",
    "config[\"maximization_metric\"] = \"average_viable_recall\"\n",
    "\n",
    "with open(os.path.join(\"training_parameters.yaml\"), \"w\") as file:\n",
    "    documents = yaml.dump(config, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoEXJBaiC9mf"
   },
   "outputs": [],
   "source": [
    "# Trains a model. When finished, it will quantize and convert the model to a\n",
    "# streaming version suitable for on-device detection.\n",
    "# It will resume if stopped, but it will start over at the configured training\n",
    "# steps in the yaml file.\n",
    "# Change --train 0 to only convert and test the best-weighted model.\n",
    "# On Google colab, it doesn't print the mini-batch results, so it may appear\n",
    "# stuck for several minutes! Additionally, it is very slow compared to training\n",
    "# on a local GPU.\n",
    "\n",
    "!python -m microwakeword.model_train_eval \\\n",
    "--training_config='training_parameters.yaml' \\\n",
    "--train 1 \\\n",
    "--restore_checkpoint 1 \\\n",
    "--test_tf_nonstreaming 0 \\\n",
    "--test_tflite_nonstreaming 0 \\\n",
    "--test_tflite_nonstreaming_quantized 0 \\\n",
    "--test_tflite_streaming 0 \\\n",
    "--test_tflite_streaming_quantized 1 \\\n",
    "--use_weights \"best_weights\" \\\n",
    "mixednet \\\n",
    "--pointwise_filters \"64,64,64,64\" \\\n",
    "--repeat_in_block  \"1, 1, 1, 1\" \\\n",
    "--mixconv_kernel_sizes '[5], [7,11], [9,15], [23]' \\\n",
    "--residual_connection \"0,0,0,0\" \\\n",
    "--first_conv_filters 32 \\\n",
    "--first_conv_kernel_size 5 \\\n",
    "--stride 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
